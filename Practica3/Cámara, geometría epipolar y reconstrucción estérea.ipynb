{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visión por Computador - Práctica 3\n",
    "## Cámara, geometría epipolar y reconstrucción estérea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimación de la matriz de una cámara a partir del conjunto de puntos en correspondencias.\n",
    "\n",
    "##### a) Generar la matriz de una cámara finita P a partir de valores aleatorios en [0,1] . Verificar si representa una cámara finita y en ese caso quedársela.\n",
    "##### b) Suponer un patrón de puntos del mundo 3D compuesto por el conjunto de puntos con coordenadas $\\{(0,x_1,x_2)$ y $(x_2,x_1,0)$, para $x_1=0.1:0.1:1$ y $x_2=0.1:0.1:1\\}$. Esto supone una rejilla de puntos en dos planos distintos ortogonales.\n",
    "##### c) Proyectar el conjunto de puntos del mundo con la cámara simulada y obtener las coordenadas píxel de su proyección.\n",
    "##### d) Implementar el algoritmo DLT para estimar la cámara P a partir de los puntos 3D y sus proyecciones en la imagen.\n",
    "##### e) Calcular el error de la estimación usando la norma de Frobenius (cuadrática).\n",
    "##### f) Mostrar en una única imagen los puntos 3D proyectados con la cámara estimada y la cámara simulada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  **Apartado a:** para generar una cámara finita $P = K[R|T]$, donde $P=[KR|KT] = [M|M_T]$ simplemente tenemos que generar una matriz con números aleatorios y asegurarnos de que el determinante de $M$ sea distinto de 0. Al final, devolveremos una matriz de tamaño $3\\times4$ con $det(M)\\neq0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import functions as fx\n",
    "\n",
    "def generate_Pcamera():\n",
    "    # La matriz cámara tiene la siguiente estructura:\n",
    "    #              P =  K[R | T]\n",
    "    # donde det(R) != 0, al igual que det(K) != 0, por\n",
    "    # lo que podemos hacer que P = [KR | KT] = [M | M_4]\n",
    "\n",
    "    # Generamos una matriz con valores aleatorios en el\n",
    "    # intervalo [0,1)\n",
    "    P_cam = np.random.rand(3,4)\n",
    "\n",
    "    # Comprobamos si det(M) != 0. En caso de que no lo\n",
    "    # sea, volvemos a generar una nueva matriz cámara.\n",
    "    while not np.linalg.det(P_cam[0:3,0:3]):\n",
    "        P_cam = np.random.rand(3,4)\n",
    "\n",
    "    P_cam = P_cam / P_cam[-1,-2]\n",
    "    return P_cam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Apartado b:** para generar los puntos con coordenadas $\\{(0,x_1,x_2)$ y $(x_2,x_1,0)$, para $x_1=0.1:0.1:1$ y $x_2=0.1:0.1:1\\}$, simplemente generamos dos arrays de 0 a 1 en intervalos de 0.1, que concatenaremos en forma de columna y añadiremos un cero por la izquierda o la derecha, según corresponda, generando puntos de la siguiente forma:\n",
    "$$set_1 = \\left[\\begin{matrix}\n",
    "  0 & 0 & 0 \\\\\n",
    "  0 & 0 & 0.1 \\\\\n",
    "   & \\vdots &  \\\\\n",
    "  0 & 0.1 & 0.5 \\\\\n",
    "  & \\vdots & \\\\\n",
    "  0 & 0.9 & 0.9\n",
    " \\end{matrix}\\right] \\qquad set_2 = \\left[\\begin{matrix}\n",
    "  0 & 0 & 0 \\\\\n",
    "  0 & 0.1 & 0 \\\\\n",
    "   & \\vdots &  \\\\\n",
    "  0.1 & 0.1 & 0 \\\\\n",
    "  & \\vdots & \\\\\n",
    "  0.9 & 0.9 & 0\n",
    " \\end{matrix}\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_points():\n",
    "    # Generamos los valores de x1 y x2\n",
    "    x1 = np.arange(start = 0, stop = 1,\n",
    "                   step = 0.1, dtype=np.float64)\n",
    "    x2 = np.arange(start = 0, stop = 1,\n",
    "                   step = 0.1, dtype=np.float64)\n",
    "    # Obtenemos una combinación de los puntos que hemos obtenido\n",
    "    points2D = np.concatenate(np.array(np.meshgrid(x1,x2)).T)\n",
    "    # Añadimos un cero por la izquierda y uno por la derecha respectivamente\n",
    "    set1 = np.hstack((np.zeros(points2D.shape[0])[...,None], points2D))\n",
    "    set2 = np.hstack((points2D, np.zeros(points2D.shape[0])[...,None]))\n",
    "    # Y devolvemos un único conjunto de puntos\n",
    "    return np.concatenate((set1, set2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Apartado c:** para proyectar los puntos del mundo sobre el espacio 2D de la retina, tenemos que multiplicar los puntos del mundo por la matriz cámara que hemos generado, y normalizar estos puntos dividiendo por la última coordenada y devolviendo la coordenada $x_i$ e $y_i$ para el punto $X_i$ obteniendo así coordenadas 2D. Para ello, es necesario tener como entrada los puntos del mundo y la cámara, además de convertir estos puntos del mundo a coordenadas homogéneas. $$X' = PX = P \\left[\\begin{matrix}\n",
    "x\\\\\n",
    "y\\\\\n",
    "z\\\\\n",
    "1\n",
    "\\end{matrix}\\right]$$ $$X'=\\left[\\begin{matrix}\n",
    "x'\\\\\n",
    "y'\\\\\n",
    "z'\n",
    "\\end{matrix}\\right] \\qquad (u,v) = \\left[\\begin{matrix}\\frac{x'}{z'}\\\\\n",
    "\\frac{y'}{z'}\\end{matrix}\\right]$$ Esta función devuelve las coordenadas homogéneas de los puntos del mundo y las coordenadas 2D de los puntos proyectados en la retina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def project_points(points, camera):\n",
    "    # Pasamos las coordenadas de los puntos a coordenadas homogéneas\n",
    "    homogeneus_points = np.hstack((points, (np.ones(points.shape[0]))[...,None]))\n",
    "    # Obtenemos una matriz vacía que serán las proyecciones\n",
    "    # de los puntos al pasar por la cámara.\n",
    "    projection = np.zeros(shape=(points.shape[0],2), dtype=np.float32)\n",
    "    # Realizamos la multiplicación\n",
    "    #    xy' = P * xy\n",
    "    for i in range(homogeneus_points.shape[0]):\n",
    "        point = np.dot(camera,homogeneus_points[i].T)\n",
    "        projection[i,0] = point[0]/point[2]\n",
    "        projection[i,1] = point[1]/point[2]\n",
    "\n",
    "    # Devolvemos las proyecciones de los puntos\n",
    "    return homogeneus_points, projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Apartado d**: para realizar el algoritmo DLT (*Direct Linear Transformation*), partiremos de que conocemos los puntos reales del mundo (puntos xyz en coordenadas homogénas) y los puntos proyectados en la retina (en coordenadas 2D, que pasaremos a coordenadas homogéneas). Antes de comenzar con el algoritmo, debemos normalizar los puntos para que el resultado del DLT no se vea afectado por estos puntos. Esto se debe a que los puntos en coordenadas del mundo y coordenadas píxeles, pueden llegar a variar en varios órdenes de magnitud, haciendo que el espacio de búsqueda del algoritmo se vea incrementado. Si normalizamos estos puntos, el espacio de búsqueda queda muchísimo más acotado, permitiendo que el algoritmo obtenga mejores soluciones que si usamos los puntos sin tratar.\n",
    "\n",
    "    Para normalizar estos puntos, se ha definido la función ```normalize```, que recibe como parámetros los puntos en coordenadas homogéneas, y para qué dimensión queremos normalizar, si 2D o 3D. Una vez aquí, calcularemos la media de los puntos y la desviación típica de estos, que usaremos para construir la matriz de normalización $N$: $$ dim = 3 \\rightarrow N = \\left(\\begin{matrix}\\sigma & 0 & 0 & \\overline{x_x}\\\\\n",
    "    0 &\\sigma  & 0 & \\overline{x_y}\\\\\n",
    "    0 & 0 & \\sigma & \\overline{x_z}\\\\\n",
    "    0 & 0 & 0 & 1\\end{matrix}\\right) \\qquad dim = 2 \\rightarrow N = \\left(\\begin{matrix}\\sigma & 0 & \\overline{x_x}\\\\\n",
    "    0 & \\sigma & \\overline{x_y}\\\\\n",
    "    0 & 0 & 1 \\end{matrix}\\right)$$\n",
    "    \n",
    "    Si calculamos $N^{-1}$ y realizamos el producto vectorial de la matriz $N^{-1}$ y los puntos, obtendremos los puntos normalizados, devolviendo solo el número de columnas iguales a la dimensión en la que estamos normalizando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Los puntos han de ser en coordenadas homogéneas\n",
    "def normalize(points, dim):\n",
    "    # Obtenemos la media de los puntos y su desviación\n",
    "    # típica para normalizar los datos\n",
    "    points_mean = np.mean(points, 0)\n",
    "    s = np.std(points[:,0:points.shape[1]-1])\n",
    "\n",
    "    # Creamos la matriz N para normalizar los puntos, esta\n",
    "    # matriz tiene la forma:\n",
    "    if dim == 2:\n",
    "        N = np.matrix([ [s, 0, points_mean[0]], [0, s, points_mean[1]], [0, 0, 1] ])\n",
    "    else:\n",
    "        N = np.matrix([[s, 0, 0, points_mean[0]], [0, s, 0, points_mean[1]], [0, 0, s, points_mean[2]], [0, 0, 0, 1]])\n",
    "\n",
    "    N = np.linalg.inv(N)\n",
    "    normalized_points = np.dot(N, points.T)\n",
    "    normalized_points = normalized_points[0:dim,:].T\n",
    "    \n",
    "    return N, normalized_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez normalizados los puntos, ya podremos continuar con el algoritmo _DLT_. Para ello, construiremos una matriz utilizando las coordenadas $x,y$ de los puntos normalizados en 2D y las coordenadas $x,y,z$ de los puntos 3D normalizados. Esta matriz tendrá la siguiente forma: $$\n",
    "\\left(\\begin{matrix}X_0 & 0 & -u_0X_0 & -u_0\\\\\n",
    "    0 & X_0 & -u_0X_0 & -v_0 \\\\\n",
    "    & \\qquad\\qquad\\vdots \\\\\n",
    "    X_n & 0 & -u_nX_n & -u_n \\\\\n",
    "    0 & X_n & -v_nX_n & -v_n \\\\\n",
    "    \\end{matrix}\\right)$$ Una vez calculada esta matriz, procedemos a calcular su descomposición en valores singulares, siendo los parámetros estimados de la cámara el autovector con menor autovalor. Este vector ha de ser normalizado y se deben deshacer los cambios que se hicieron para a la hora de normalizar los puntos, para obtener la matriz $P$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Algoritmo DLT para obtener una cámara estimada a partir\n",
    "# de los puntos en el mundo y los puntos de la retina\n",
    "def DLT_algorithm(real_points, projected_points, camera):\n",
    "    # Normalizamos los puntos para mejorar el resultado\n",
    "    # del algoritmo DLT\n",
    "    N_matrix, normalized_points = normalize(real_points, 3)\n",
    "    homogeneus_proj_pt = np.hstack((projected_points, (np.ones(projected_points.shape[0]))[...,None]))\n",
    "    N_matrix2d, norm_points_2d = normalize(homogeneus_proj_pt, 2)\n",
    "    # Recorremos todos los puntos 3D que tenemos\n",
    "    # y generamos una matriz M con todos los puntos\n",
    "    aux = []\n",
    "    for i in range(normalized_points.shape[0]):\n",
    "        x_i, y_i, z_i = normalized_points[i,0], normalized_points[i,1], normalized_points[i,2]\n",
    "        u, v = norm_points_2d[i,0], norm_points_2d[i,1]\n",
    "        aux.append([x_i, y_i, z_i, 1, 0, 0, 0, 0, -u*x_i, -u*y_i, -u*z_i, -u])\n",
    "        aux.append([0, 0, 0, 0, x_i, y_i, z_i, 1, -v*x_i, -v*y_i, -v*z_i, -v])\n",
    "\n",
    "    # Descomponemos la matriz\n",
    "    U, s, V = np.linalg.svd(np.array(aux, dtype=np.float64))\n",
    "    # Obtenemos los parámetros\n",
    "    camera_estimated = V[-1,:]/V[-1,-2]\n",
    "    camera_estimated = np.matrix(camera_estimated).reshape(3,4)\n",
    "    # Desnormalizamos\n",
    "    camera_estimated = np.dot(np.dot(np.linalg.pinv(N_matrix2d), camera_estimated), N_matrix)\n",
    "    camera_estimated = camera_estimated/camera_estimated[-1,-2]\n",
    "    # Calculamos el error de la cámara estimada\n",
    "    error = np.linalg.norm(x=(camera - camera_estimated), ord=None)\n",
    "    \n",
    "    return camera_estimated, error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Apartado e**: el error, según la norma de Frobenius consiste en: $$||E|| =  \\sqrt{\\sum_{i=1}^m\\sum_{j=1}^n abs(P_{real_{ij}} - P_{estimada_{ij}})^2}$$ La norma de Frobenius se encuentra implementada en la librería *NumPy*, y podemos acceder a ella con la función ```np.linalg.norm``` con el parámtro *ord* con valor *None*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.57968616  1.28509819  0.90435767  1.02369777]\n",
      " [ 0.83382207  1.72743251  1.69327598  0.37200569]\n",
      " [ 1.66771089  0.8471442   1.          0.82590861]]\n",
      "[[ 0.57968613  1.28509803  0.90435767  1.02369771]\n",
      " [ 0.83382201  1.72743236  1.69327593  0.37200567]\n",
      " [ 1.66771077  0.84714408  1.          0.82590856]]\n",
      "2.95601862058e-07\n"
     ]
    }
   ],
   "source": [
    "camera = generate_Pcamera()\n",
    "points = generate_points()\n",
    "hom_points, projected = project_points(points, camera)\n",
    "camera_est, err = DLT_algorithm(real_points=hom_points, projected_points=projected, camera=camera)\n",
    "print(camera)\n",
    "print(camera_est)\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar en los resultados del algoritmo, la camara estimada es muy similar a la original, tan solo habiendo cambios en algunos elementos a partir de la sexta cifra decimal, obteniendo un error siguiendo la norma de Frobenius de $10^-7$, siendo un error muy pequeño."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibración de la cámara usando homografías\n",
    "\n",
    "#### a) Escribir una función que sea capaz de ir leyendo las sucesivas imágenes en chessboard.rar y determine cuáles son válidas para calibrar una cámara. Usar las 25 imágenes tiff que se incluyen en el fichero datos. Usar la función + ```cv::findChessboardCorners()```. Determinar valores precisos de las coordenadas de las esquinas presentes en las imágenes seleccionadas usando ```cv::cornerSubpix()```. Pintar sobre la imagen los puntos estimados usando la función ```cv::drawChessboardCorners()```.\n",
    "\n",
    "#### b) Usando las coordenadas de los puntos extraídos en las imágenes seleccionadas del punto anterior, calcular los valores de los parámetros intrínsecos y extrínsecos de la cámara para cada una de dichas imágenes. Usar la función ```cv::calibrateCamera()```. Suponer dos situaciones: a) sin distorsión óptica y b) con distorsión óptica. Valorar la influencia de la distorsión óptica en la calibración y la influencia de la distorsión radial frente a la distorsión tangencial.\n",
    "\n",
    "* **Apartado a:** Para resolver este ejercicio, se ha creado la función ```calibrate_camera_from``` que recibe como parámetros las 25 imágenes con las que vamos a trabajar. Llamando al la función ```findChessboardCorners``` obtenemos cuáles son válidas para calibrar una cámara y cuales no, junto con las coordenadas 2D de los puntos. Como requisito para poder calibrar una cámara, es necesario que al menos haya 4 imágenes distintas. Una vez obtenidos los puntos refinados con ```cornerSubpix``` para poder calcular la calibración de la cámara. \n",
    "\n",
    "    En las imágenes que se muestran a continuación, podemos ver el resultado de la función ```drawChessboardCorners``` donde nos dibuja el patrón de puntos que ha encontrado que son útiles para el calibrado de la cámara.\n",
    "\n",
    "* **Apartado b:** Para poder calibrar las cámaras, tenemos que realizarlo en referencia a unas coordenadas del mundo. Al no saber las coordenadas exactas del mundo, asumiremos nosotros nuestro propio origen de coordenadas y las unidades de distancia en el mundo que deseemos para poder trabajar. \n",
    "\n",
    "    Al llamar a la función ```calibrateCamera``` necesitamos los puntos de los patrones extraídos en cada una de las imágenes y los puntos del mundo, junto con el tamaño de una de las imágenes. Esta función devolverá el **reprojection_error**, la matriz **cámara**, los **coeficientes de distorsión**, la **matriz de rotación** y la **matriz de translación**. Esta matriz cámara no tiene en cuenta la distorsión que puede ofrecer el objetivo o la lente de cámara.\n",
    "\n",
    "    Para anular la distorsión que producen las lentes, primero refinaremos la matriz cámara con la función ```getOptimalNewCameraMatrix``` que refina la matriz cámara y nos devuelve el rectángulo válido de la imagen sin distorsión. Una vez que tenemos la matriz refinada, podemos llamar a la función ```undistort``` para revertir el efecto de la lente y quitar la distorsión que produce la lente. Para ello, necesitamos los coeficientes de distorsión que hemos extraído anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calibrate_camera_from(images, use_lenss = False, alpha = 1):\n",
    "    valids = []\n",
    "    size = (13, 12)\n",
    "    # Seleccionamos los flags que vamos a usar\n",
    "    cv2_flags =  cv2.CALIB_CB_NORMALIZE_IMAGE | cv2.CALIB_CB_ADAPTIVE_THRESH | cv2.CALIB_CB_FAST_CHECK | cv2.CALIB_CB_FILTER_QUADS\n",
    "    # Creamos nosotros las coordenadas del mundo para poder\n",
    "    # compararlas con los puntos de las imágenes \n",
    "    # y generar la cámara\n",
    "    world_points =  np.zeros((13*12,3), np.float32)\n",
    "    world_points[:,:2] = np.mgrid[0:13,0:12].T.reshape(-1,2)\n",
    "    \n",
    "    for img in images:\n",
    "        valids.append(cv2.findChessboardCorners(img, size, flags=cv2_flags))\n",
    "        if valids[-1][0]:\n",
    "        # Si la imagen es válida, procedemos a refinar\n",
    "        # los puntos con cornerSubPix\n",
    "            cv2.cornerSubPix(image=img.astype(np.float32), corners=valids[-1][1],\n",
    "                             winSize=(5, 5), zeroZone=(-1, -1),\n",
    "                             criteria=(cv2.TERM_CRITERIA_MAX_ITER | cv2.TERM_CRITERIA_COUNT, 30, 0.001))\n",
    "    # Coordenadas de las imágenes seleccionadas\n",
    "    coordinates = []\n",
    "    worldP = []\n",
    "    valid_images = []\n",
    "    for i in range(0,len(valids)):\n",
    "        # Si es un punto válido:\n",
    "        if valids[i][0]:\n",
    "            # Mostramos el patrón de puntos encontrado\n",
    "            fx.show_img(cv2.drawChessboardCorners(image = cv2.cvtColor(images[i], cv2.COLOR_GRAY2BGR), \n",
    "                                                  patternSize = size,\n",
    "                                                  corners = valids[i][1], \n",
    "                                                  patternWasFound = valids[i][0]),\n",
    "                        \"imagen \"+str(i))\n",
    "            # Almacenamos las coordenadas de los puntos que forman el\n",
    "            # patrón para calibrar la cámara\n",
    "            coordinates.append(valids[i][1])\n",
    "            # Las coordenadas del mundo para formar las correspondencias\n",
    "            worldP.append(world_points)\n",
    "            # Y guardamos las imágenes válidas\n",
    "            valid_images.append(images[i])\n",
    "    \n",
    "    # Tras esto, llamamos a calibrateCamera para calibrar la\n",
    "    # cámara a partir de las coordenadas del mundo y las del \n",
    "    # patrón\n",
    "    reprojection_error, camera, distorsion_coefs, rotation_vecs, translation_vecs = cv2.calibrateCamera(worldP, \n",
    "                                                           coordinates, \n",
    "                                                           valid_images[-1].shape[::-1],\n",
    "                                                           None,None)\n",
    "    print(\"reprojection_error = \", reprojection_error)\n",
    "    print(\"camera = \\n\", camera)\n",
    "    print(\"distorsion coeffs = \", distorsion_coefs)\n",
    "    print(\"rotation vecs = \\n\", rotation_vecs)\n",
    "    print(\"tvecs = \\n\", translation_vecs)\n",
    "    \n",
    "    if use_lenss:\n",
    "        # si el parámetro use_lenss está activo, vamos a proceder\n",
    "        # a quitar la distorsión producida por las lentes. Primero\n",
    "        # refinamos la cámara obtenida anteriormente\n",
    "        height, width = valid_images[-1].shape[:2]\n",
    "        # Devolvemos la cámara y el rectángulo óptimo de píxeles para\n",
    "        # evitar la distorsión\n",
    "        ref_cam, valid_rectangle = cv2.getOptimalNewCameraMatrix(camera, distorsion_coefs, \n",
    "                                                                 (width, height), alpha, \n",
    "                                                                 (width, height))\n",
    "        print(\"refined camera = \\n\", ref_cam)\n",
    "        # Una vez que hemos obtenido la cámara refinada\n",
    "        # pasamos a rectificar la distorsión.\n",
    "        correct_image = cv2.undistort(src=valid_images[-1], cameraMatrix=camera, \n",
    "                                   distCoeffs=distorsion_coefs, dst=None,\n",
    "                                   newCameraMatrix=ref_cam)\n",
    "        # Obtenemos por separado los valores de la cuaterna\n",
    "        # para trabajar más fácilmente.\n",
    "        fx.show_img(correct_image, 'Resultado con lentes')    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimación de la matriz fundamental *F*\n",
    "\n",
    "##### a) Obtener puntos en correspondencias sobre las imágenes Vmort[*].pgm de forma automática usando las funciones de BRISK/ORB.\n",
    "##### b) Calcular F por el algoritmo de los 8 puntos + RANSAC (usar un valor pequeño para el error de RANSAC)\n",
    "##### c) Dibujar las líneas epipolares sobre ambas imágenes. \n",
    "##### d) Verificar la bondad de la F estimada calculando la media de la distancia ortogonal entre los puntos soporte y sus líneas epipolares en ambas imágenes. Mostrar el valor medio del error.\n",
    "\n",
    "a) Para obtener los puntos en correspondencia de las imágenes he definido la función ```get_matches```. Esta función recibe como parámetros las dos imágenes *Vmort* y obtiene los *keypoints* y sus descriptores usando BRISK y ORB. Una vez extraídos los keypoints, obtiene los puntos en correspondencia usando un *matcher* que utiliza la fuerza bruta y la validación cruzada para ambos descriptores. Al final la función compara el número de matches que se han obtenido usando BRISK y usando ORB, devolviendo los matches, keypoints y descriptores del que haya obtenido más puntos en correspondencia. Además, la función mostrará en una primera imagen todos los matches obtenidos y en una segunda el 15% mejor de los matches obtenidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_matches(image1, image2, show_imgs = True):\n",
    "    # Vamos a inicializar un dectector ORB \n",
    "    # y un detector BRISK, y dejaremos aquel que obtenga\n",
    "    # más puntos\n",
    "    orb_detector = cv2.ORB_create()\n",
    "    brisk_detector = cv2.BRISK_create()\n",
    "    # Buscamos los keypoints y los descriptores de ambas\n",
    "    # imágenes haciendo uso de ORB\n",
    "    keyP1_orb, des1_orb = orb_detector.detectAndCompute(image1,None)\n",
    "    keyP2_orb, des2_orb = orb_detector.detectAndCompute(image2,None)\n",
    "    # Buscamos los keypoints y los descriptores de ambas\n",
    "    # imágenes haciendo uso de BRISK\n",
    "    keyP1_brisk, des1_brisk = brisk_detector.detectAndCompute(image1,None)\n",
    "    keyP2_brisk, des2_brisk = brisk_detector.detectAndCompute(image2,None)\n",
    "    # Inicializamos el BFMatcher con la norma Hamming para ORB y BRISK\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    \n",
    "    # Hacemos matching entre los descriptores de ORB\n",
    "    matches_orb = bf.match(des1_orb, des2_orb)\n",
    "    # y para los de BRISK\n",
    "    matches_brisk = bf.match(des1_brisk, des2_brisk)\n",
    "    # Calculamos cuál ha obtenido más puntos en\n",
    "    # correspondencias y nos quedamos con sus matches\n",
    "    if len(matches_orb) > len(matches_brisk):\n",
    "        matches = matches_orb\n",
    "        kp1, kp2 = keyP1_orb, keyP2_orb\n",
    "        des1, des2 = des1_orb, des2_orb\n",
    "        print(\"Puntos en corresponencia usando ORB\")\n",
    "        print(\"Total de puntos: \", len(matches))\n",
    "        \n",
    "    else:\n",
    "        matches = matches_brisk\n",
    "        kp1, kp2 = keyP1_brisk, keyP2_brisk\n",
    "        des1, des2 = des1_brisk, des2_brisk\n",
    "        print(\"Puntos en corresponencia usando BRISK\")\n",
    "        print(\"Total de puntos: \", len(matches))\n",
    "    \n",
    "    if show_images:\n",
    "        img_match = cv2.drawMatches(img1 = image1, keypoints1 = kp1,\n",
    "                                    img2 = image2, keypoints2 = kp2, \n",
    "                                    matches1to2 = matches, outImg = None, flags=2)\n",
    "        sorted_kp_img_match = cv2.drawMatches(img1 = image1, keypoints1 = kp1, \n",
    "                                              img2 = image2, keypoints2 = kp2, \n",
    "                                              matches1to2 = sorted(matches, key = lambda x:x.distance)[0:int(len(matches)*0.15)], \n",
    "                                              outImg = None, flags=2)\n",
    "\n",
    "        fx.show_img(img_match, 'Todos los puntos en corresponencias')    \n",
    "        fx.show_img(sorted_kp_img_match, 'El 15% de mejores puntos en corresponencias')  \n",
    "    \n",
    "    return matches, kp1, des1, kp2, des2\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Apartado b:** Según el libro *Multiple View Geometry*, dadas dos imágenes distintas de una misma escena, codifican una geometría epipolar que es independiente de la escena en sí, y sólo dependen de los parámetros internos de la cámara y su posición relativa. Esta geometría epipolar la codifica la matriz fundamental $F$, que es una matriz $3\\times3$ con rango 2. Esta matriz fundamental satisface que $$x'^TFx = 0$$ donde $x$ y $x$ son puntos en correspondencias entre ambas imágenes, siendo una ecuación muy similar a la del cálculo de la homografía. Además, si la matriz $F$ está multiplicada por una constante es irrelevante ya que aun así codifica la misma geometría.\n",
    "\n",
    "    Partiendo de esta ecuación, si disponemos de al menos 7 puntos en correspondencias entre ambas imágenes, podemos usarlos para determinar la matriz $F$ desconocida. Para empezar, pondremos los puntos como $x = (x,y,1)^T$ y $x' = (x',y',1)^T$. Siguiendo con esto y la ecuación $x'^TFx = 0$, tenemos que para cada punto tenemos una ecuación como la siguiente: $$x'xf_{11} + x'yf_{12} + x'f_{13}+y'xf_{21}+y'yf_{22} + y'f_{23} + xf_{31} + yf_{32} + f_{33} = 0$$\n",
    "    Esto a su vez puede expresarse como: $$(x'x,x'y,x',y'x,y'y,y',x,y,1)f = 0$$\n",
    "\n",
    "    Por tanto, si tenemos $n$ puntos tendremos que: $$Af = \\left(\\begin{matrix}\n",
    "    x'_1x_1 & x'_1y_1 & x'_1 & y'_1x_1 & y'_1y_1 & y'_1 & x_1 & y_1 & 1\\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\\n",
    "    x'_nx_n & x'_ny_n & x'_n & y'_nx_n & y'_ny_n & y'_n & x_n & y_n & 1\\\\\n",
    "    \\end{matrix}\\right)f=0$$\n",
    "    \n",
    "    Además de esto, la matriz $F$ tiene 2 restricciones:\n",
    "    * **$det(F) = 0$**\n",
    "    * **La constante que multiplica a F es irrelevante**\n",
    "    \n",
    "Luego solo se necesitan 7 ecuaciones para determinar $F$. Para encontrar la matriz $F$, en vez de resolver el sistema, minimizamos $||Af||$ y para ello acudimos a la descomposición en valores singulares con el fin de obtener el autovector con menor autovalor de $A^TA$.\n",
    "\n",
    "El método de los 8-puntos sustituye la matriz $F$ por una matriz $F'$ e intenta minimizar $||F - F'||$.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimate_fundamental_matrix_from(image1, image2):\n",
    "    # Obtenemos los puntos en correspondencias\n",
    "    matches, kp1, des1, kp2, des2 = get_matches(image1, image2)\n",
    "    img_points1 = []\n",
    "    img_points2 = []\n",
    "    # Recuperamos las coordenadas de los puntos en correspondencias:\n",
    "    for match in matches:\n",
    "        img_points1.append(kp1[match.queryIdx].pt)\n",
    "        img_points2.append(kp2[match.trainIdx].pt)\n",
    "        \n",
    "    img_points1 = np.array(img_points1, dtype=np.int32)\n",
    "    img_points2 = np.array(img_points2, dtype=np.int32)\n",
    "    # Psamos a obtener la matriz fundamental con el \n",
    "    # algoritmo de los 8 puntos usando RANSAC\n",
    "    fundamental_mat, mask = cv2.findFundamentalMat(points1 = img_points1, \n",
    "                                             points2 = img_points2,\n",
    "                                             method = cv2.FM_8POINT + cv2.FM_RANSAC, \n",
    "                                             param1 = 10**-2,\n",
    "                                             param2 = 0.9999999)\n",
    "    \n",
    "    # Descartamos los puntos que son outliers\n",
    "    img_points1 = img_points1[mask.ravel()==1]\n",
    "    img_points2 = img_points2[mask.ravel()==1]\n",
    "    print(\"Matriz fundamental F:\\n\",fundamental_mat)\n",
    "    \n",
    "    return fundamental_mat, img_points1, img_points2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Apartado c**: para dibujar las líneas epipolares, he definido la función ```show_epilines``` que recibe ambas imágenes y los *keypoints* de cada una de ellas y la matriz fundamental. Esta función hace uso de la función de *OpenCV* ```computeCorrespondEpilines``` que se encarga de obtener las líneas epipolares para cada una de las imágenes. Tras esto, con la función ```draw_epilines``` que he definido, muestro en una misma imagen las líneas epipolares en una de las dos imágenes y en la otra los puntos a los que corresponden esas líneas epipolares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_epilines(image1, img_points1, image2, img_points2, epilines):\n",
    "    # Pasamos las imágenes de escala de grises a color para\n",
    "    # poder representar las líneas epipolares de una manera\n",
    "    # más clara\n",
    "    aux_img1 = cv2.cvtColor(image1, cv2.COLOR_GRAY2BGR)\n",
    "    aux_img2 = cv2.cvtColor(image2, cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "    for i in range(min(len(epilines),200)):\n",
    "        # Generamos un color aleatorio\n",
    "        line_color = tuple(np.random.randint(0,255,3).tolist())\n",
    "        init_point = (0, int(-epilines[i][2]/epilines[i][1]))\n",
    "        end_point = (image1.shape[1], int(-(epilines[i][2]+epilines[i][0]*image1.shape[1])/epilines[i][1]))\n",
    "        # al tener definidos los dos puntos, podemos \n",
    "        # crear la línea epipolar que pasa por esos \n",
    "        # dos puntos\n",
    "        aux_img1 = cv2.line(img = aux_img1, pt1 = init_point, \n",
    "                            pt2 = end_point, color = line_color,\n",
    "                            thickness = 2)\n",
    "        aux_img1 = cv2.circle(img = aux_img1, \n",
    "                              center=tuple(img_points1[i].astype(np.int64)), \n",
    "                              radius = 3, color = line_color)\n",
    "        aux_img2 = cv2.circle(img = aux_img2, \n",
    "                              center=tuple(img_points2[i].astype(np.int64)), \n",
    "                              radius = 3, color = line_color)\n",
    "    \n",
    "    return aux_img1, aux_img2\n",
    "    \n",
    "def show_epilines(image1, img_points1, image2, img_points2, fundamental_mat):\n",
    "    # Obtenemos las epilineas de ambas imágenes\n",
    "    epipolarline_img1 = cv2.computeCorrespondEpilines(img_points1, 1, fundamental_mat).reshape(-1,3)\n",
    "    epipolarline_img2 = cv2.computeCorrespondEpilines(img_points2, 2, fundamental_mat).reshape(-1,3)\n",
    "    # Dibujamos las líneas epipolares\n",
    "    # Lineas epipolares de la primera imagen sobre la segunda\n",
    "    epip1, epip2 = draw_epilines(image1, img_points1, image2, img_points2, epipolarline_img2)\n",
    "    canvas1 = np.zeros((epip1.shape[0],epip1.shape[1]+epip2.shape[1], 3), dtype = np.uint8)\n",
    "    fx.insert_img_into_other(img_src=epip2, img_dest=canvas1,\n",
    "                          pixel_left_top_row=0, pixel_left_top_col=0,\n",
    "                          substitute=True)\n",
    "    fx.insert_img_into_other(img_src=epip1, img_dest=canvas1,\n",
    "                          pixel_left_top_row=0, pixel_left_top_col=epip1.shape[1],\n",
    "                          substitute=True)\n",
    "    \n",
    "    # Lineas epipolares de la segunda imagen sobre la primera\n",
    "    epip3, epip4 = draw_epilines(image2, img_points2, image1, img_points1, epipolarline_img1)\n",
    "    canvas2 = np.zeros((epip3.shape[0],epip3.shape[1]+epip4.shape[1], 3), dtype = np.uint8)\n",
    "    fx.insert_img_into_other(img_src=epip3, img_dest=canvas2,\n",
    "                          pixel_left_top_row=0, pixel_left_top_col=0,\n",
    "                          substitute=True)\n",
    "    fx.insert_img_into_other(img_src=epip4, img_dest=canvas2,\n",
    "                          pixel_left_top_row=0, pixel_left_top_col=epip1.shape[1],\n",
    "                          substitute=True)\n",
    "    # Mostramos ambas imágenes\n",
    "    fx.show_img(canvas1, 'Epilineas')\n",
    "    fx.show_img(canvas2, 'Epilineas')\n",
    "    \n",
    "    return epipolarline_img1, epipolarline_img2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En las imágenes podemos ver cómo las líneas los puntos de una imagen recaen sobre la línea epipolar dibujada en la otra imagen, con lo que podemos ver que la matriz fundamental que hemos calculado, junto con la línea epipolar calculada es bastante precisa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Apartado d:** para calcular el error seguiremos el cálculo del error epipolar simétrico: $$error = \\frac{\\frac{\\sum_{i=0}^n dst(pt1_i, epiline_1)}{n} + \\frac{\\sum_{i=0}^n dst(pt2_i, epiline_2)}{n}}{2}$$ y para calcular la distancia de un punto $p_i = (x_i, y_i)$ a su línea epipolar $l = ax + by + c$  hacemos: $$ d = \\frac{|ax_i + by_i +c|}{\\sqrt{a^2 + b^2}}$$ que podemos ver en el siguiente enlace de [Wikipedia](https://en.wikipedia.org/wiki/Distance_from_a_point_to_a_line). De esto se encarga la función ```epipolar_line_error```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Para calcular la bondad de F, usaremos el error\n",
    "# epipolar simétrico\n",
    "def epipolar_line_error(pts_im1, pts_im2, line_1, line_2):\n",
    "    abs_value = math.fabs\n",
    "    sqrt = math.sqrt\n",
    "    # Función que calcula la distancia de un punto a una recta\n",
    "    dst = lambda line, point: abs_value((line[0]*point[0] + line[1]*point[1] \n",
    "                                         + line[2])/sqrt(line[0]**2 + line[1]**2))\n",
    "    \n",
    "    dst_pt1_to_line1 = []\n",
    "    dst_pt2_to_line2 = []\n",
    "    # Recorremos los puntos calculando las distancias \n",
    "    # del punto a la línea\n",
    "    for i in range(len(pts_im1)):\n",
    "        dst_pt1_to_line1.append(dst(line_1[i], pts_im2[i]))\n",
    "        dst_pt2_to_line2.append(dst(line_2[i], pts_im1[i]))\n",
    "    \n",
    "    # Calculamos el error:\n",
    "    F_error = (np.mean(dst_pt1_to_line1) + np.mean(dst_pt2_to_line2))/2\n",
    "    print(\"Error de F: \", F_error)\n",
    "    return F_error\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
